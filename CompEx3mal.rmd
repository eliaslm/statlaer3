---
subtitle: "TMA4268 Statistical Learning V2018"
title: "Compulsory exercise 3: Group XYZ"
author: "Huglen, Huso and Myklebust"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results="hold",message = FALSE,warning=FALSE)
```

```{r,echo=FALSE,eval=FALSE}
library(caret) 
#read data, divide into train and test
germancredit = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data")
colnames(germancredit) = c("checkaccount", "duration", "credithistory", "purpose", "amount", "saving", "presentjob", "installmentrate", "sexstatus", "otherdebtor", "resident", "property", "age", "otherinstall", "housing", "ncredits", "job", "npeople", "telephone", "foreign", "response")
germancredit$response = as.factor(germancredit$response) #2=bad
table(germancredit$response)
str(germancredit) # to see factors and integers, numerics

set.seed(4268) #keep this -easier to grade work
in.train <- createDataPartition(germancredit$response, p=0.75, list=FALSE)
# 75% for training, one split
germancredit.train <- germancredit[in.train,]; dim(germancredit.train)
germancredit.test <- germancredit[-in.train,];dim(germancredit.test)
```

## 1a) Full classification tree

* Q1. The tree is contructed by using recursive binary splitting. This is a top-down approach because it begins at the root of the tree, successively splitting predictor space into two and two new branches as it moves down the tree. This method is also greedy because it takes the locally best choice by making the best split at each level. The terminal nodes, the nodes at the bottom of the tree, are also known as leaves. Here, the predicted class for the observation is decided. The fact that the leaves are class labels is what makes the tree a classification tree.  The deviance is a measurement of how good the fit is, basically by measuring the distance between a "perfect"" model (a model which explains all the variance in the response) and a candidate. One wishes to split in a way such that the deviance is  minimized. Popular splitting criteria for the classification tree are the Gini index and cross-entropy.

```{r,echo=FALSE,eval=FALSE}
# construct full tree
library(tree)
library(pROC)
fulltree=tree(response~.,germancredit.train,split="deviance")
summary(fulltree)
plot(fulltree)
text(fulltree)
print(fulltree)
fullpred=predict(fulltree,germancredit.test,type="class")
testres=confusionMatrix(data=fullpred,reference=germancredit.test$response)
print(testres)
1-sum(diag(testres$table))/(sum(testres$table))
predfulltree = predict(fulltree,germancredit.test, type = "vector")
testfullroc=roc(germancredit.test$response == "2", predfulltree[,2])
auc(testfullroc)
plot(testfullroc)
```


## b) Pruned classification tree 

* Q2. The model includes alot of predictors. That means that the observations from a dedicated training set may be partitioned into equally many small regions, and the probability of having overfitting increases. Pruning helps avoiding this by reducing the depth of the tree and thus reducing the amount of regions.
* Q3. The amount of pruning is decided by using cross-validation on the full tree model, in this case with 5-fold-CV. The classification error rate is used as evaluation for the cross-validation procedure. We pick the number of nodes in the model with the lowest error rate. This is the number of nodes in the pruned model.   How is amount of pruning decided in the code?
* Q4. As the complexity of the model decreases with the pruned tree, it also becomes more interpretable. However, the AUC is lower for the pruned tree than for the full tree, and the misclassification rate has also increased.??  Compare the the full and pruned tree classification method with focus on interpretability and the ROC curves (AUC).

```{r, echo=FALSE, eval=FALSE}
# prune the full tree
set.seed(4268)
fullcv=cv.tree(fulltree,FUN=prune.misclass,K=5)
plot(fullcv$size,fullcv$dev,type="b", xlab="Terminal nodes",ylab="misclassifications")
print(fullcv)
prunesize=fullcv$size[which.min(fullcv$dev)]
prunetree=prune.misclass(fulltree,best=prunesize) 
plot(prunetree)
text(prunetree,pretty=1)
predprunetree = predict(prunetree,germancredit.test, type = "class")
prunetest=confusionMatrix(data=predprunetree,reference=germancredit.test$response)
print(prunetest)
1-sum(diag(prunetest$table))/(sum(prunetest$table))
predprunetree = predict(prunetree,germancredit.test, type = "vector")
testpruneroc=roc(germancredit.test$response == "2", predprunetree[,2])
auc(testpruneroc)
plot(testpruneroc)
```

## c) Bagged trees 

* Q5. The main motivation behind bagging is to decrease the amount of variance in the decision tree. What is the main motivation behind bagging?
* Q6. Variable importance plots visualize the relative importance of each of the predictors in the model. The higher the variables are ranked, the higher is their importance. The importance is interpreted as "total decrease in node impurity over splits for the predictors". Impurity here means the presence of observations from more than a single class. For this data set we observe that "checkaccount" and "amount" are important predictors, dependent of how we calculate the importance. The least important predictor is in both cases the "foreign". In general, we see that the ranking depends on whether we use the Gini Index or "Decrease accuracy" for calculating the importance. Explain what the importance plots show, and give your interpretation for the data set.
* Q7. Compare the performance of bagging with the best of the full and pruned tree model above with focus on interpretability and the ROC curves (AUC). ?????

```{r,echo=FALSE,eval=FALSE}
library(randomForest)
set.seed(4268)
bag=randomForest(response~., data=germancredit,subset=in.train,
                 mtry=20,ntree=500,importance=TRUE)
bag$confusion
1-sum(diag(bag$confusion))/sum(bag$confusion[1:2,1:2])
yhat.bag=predict(bag,newdata=germancredit.test)
misclass.bag=confusionMatrix(yhat.bag,germancredit.test$response)
print(misclass.bag)
1-sum(diag(misclass.bag$table))/(sum(misclass.bag$table))
predbag = predict(bag,germancredit.test, type = "prob")
testbagroc=roc(germancredit.test$response == "2", predbag[,2])
auc(testbagroc)
plot(testbagroc)
varImpPlot(bag,pch=20)
```

## d) Random forest 

* Q8. The parameter `mtry=4` is used. What does this parameter mean, and what is the motivation behind choosing exactly this value?
* Q9. The value of the parameter `mtry` is the only difference between bagging and random forest. What is the effect of choosing `mtry` to be a value less than the number of covariates?
* Q10. Would you prefer to use bagging or random forest to classify the credit risk data?

```{r,echo=FALSE,eval=FALSE}
set.seed(4268)
rf=randomForest(response~.,
                 data=germancredit,subset=in.train,
                 mtry=4,ntree=500,importance=TRUE)
rf$confusion
1-sum(diag(rf$confusion))/sum(rf$confusion[1:2,1:2])
yhat.rf=predict(rf,newdata=germancredit.test)
misclass.rf=confusionMatrix(yhat.rf,germancredit.test$response)
print(misclass.rf)
1-sum(diag(misclass.rf$table))/(sum(misclass.rf$table))
predrf = predict(rf,germancredit.test, type = "prob")
testrfroc=roc(germancredit.test$response == "2", predrf[,2])
auc(testrfroc)
plot(testrfroc)
varImpPlot(rf,pch=20)
```

# Problem 2 - Nonlinear class boundaries and support vector machine

## a) Bayes decision boundary 

* Q11. A Bayes classifier is a classification rule that classifies an observation to the class $k$ for which $Pr(Y = k | X = x)$ is the greatest. That is, we classify to the class for wich the probability is the greatest, given the observed values for x. A Bayes decision boundary consists of the points where there is an equal chance of an observation being in either of the classes. In a two class setting this corresponds to the points for which there is a 50% chance of an observation being in either class. In a classification setting the test error rate is defined as the average number of misclassifications on a test set. The Bayes classifier achieves the minimum of this error rate, and this is vallet the Bayes error rate. Since the Bayes classifier assigns an observation to the class with the highest probability, we can compute the Bayes error rate as $1 - E(max_k Pr(Y = k | X))$. Here the expectation is taken over all values of X. The Bayes error rate minimizes the test error rate, and is therefore analogous to irreducible error.
* Q12. If the Bayes decision boundary is known, we do not need a test set since we already know how to classify the observations in order to minimize test error. Thus we already have the best classification method.

## b) Support vector machine

* Q13. What is the difference between a support vector classifier and a support vector machine?
* Q14. What are parameters for the support vector classifier and the support vector machine? How are these chosen above?
* Q15. How would you evaluate the support vector machine decision boundary compared to the Bayes decision boundary?

# Problem 3 - Unsupervised methods

## a) Principal component analysis 

* Q16. Explain what you see in the `biplot` in relation to the loadings for the first two principal components. 
* Q17. Does this analysis give you any insight into the consumption of beverages and similarities between countries? 

```{r,echo=FALSE,eval=FALSE}
# reading data on consumption of different beverages for countries
drink <- read.csv("https://www.math.ntnu.no/emner/TMA4267/2017v/drikke.TXT",sep=",",header=TRUE)
drink <- na.omit(drink)
# looking at correlation between consumptions
drinkcorr=cor(drink)
library(corrplot)
corrplot(drinkcorr,method="circle")
# now for PCA
pcaS <- prcomp(drink,scale=TRUE) # scale: variables are scaled 
pcaS$rotation
summary(pcaS)
biplot(pcaS,scale=0,cex=0.6) # scale=0: arrows scaled to represent the loadings
```

## b) Hierarchical clustering 

* Q18. Describe how the distance between _clusters_ are defined for single, complete and average linkage. 
* Q19. Identify which of the three dendrograms (A, B, C) correspond to the three methods single, complete and average linkage. Justify your solution.

# Problem 4 - Neural networks

* Q20. What is the advantage of using a non-linear activation function such as `relu`?
* Q21. Why do we need to use a different activation function (`sigmoid`) in the output layer instead of using `relu` again? 
* Q22. Plot the training and validation loss and accuracy for the simpler and more complex model mentioned above. How do they compare with the model with 16 hidden units?
* Q23. Besides reducing the network's size, what other methods can be used to avoid overfitting with neural network models? Briefly describe the intuition behind each one.




